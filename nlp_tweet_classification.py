# -*- coding: utf-8 -*-
"""NLP tweet classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXR55Fcw-xIFjqRDI_u8Or625qlRbxbF
"""

import torch
import torch.nn as nn
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import pandas as pd
my_df =pd.read_csv("cyberbullying_tweets.csv")
corpus = my_df['tweet_text']
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('age', 0.0)
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('ethnicity', 1.0)
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('religion', 2.0)
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('gender', 3.0)
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('other_cyberbullying', 4.0)
my_df['cyberbullying_type'] = my_df['cyberbullying_type'].replace('not_cyberbullying', 5.0)
labels = my_df['cyberbullying_type']

"""Using the tfidf vectorizer

"""

#Max features are kept to 10,000 only eve though about 78,000 are present as the current specs give cannot afford more than this.
#Stop words are the words that are repeated like 'is', 'and' etc.
#ngram words are repeated words, but we are now looking at not only one words, but instead can also be 2 at a time.
vectorizer = TfidfVectorizer(max_features = 10000, stop_words='english', ngram_range=(1, 2))
X = vectorizer.fit_transform(my_df['tweet_text']).toarray()
y = my_df['cyberbullying_type'].values

"""Using a simple count vector"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features = 10000, stop_words='english', ngram_range=(1, 2))
X = vectorizer.fit_transform(my_df['tweet_text']).toarray()
y = my_df['cyberbullying_type'].values

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = torch.tensor(X_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.long)
y_test = torch.tensor(y_test, dtype=torch.long)

"""# **THE SIMPLE ANN IMPLEMENTATION**

A simple ANN, using linear transformation
"""

class SimpleANN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

input_dim = X_train.shape[1]  # input set as how many input features we have.
hidden_dim = 100
output_dim = len(set(y))  # Number of output classes hat we can set

model = SimpleANN(input_dim, hidden_dim, output_dim)

"""CrossEntropyLoss used as the loss function and Adam is used as the optimizer"""

#This loss fucntion is used as it is good for multi-class classifications
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
losses = []
epochs = 20
for epoch in range(epochs):
    model.train()

    optimizer.zero_grad()

    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')

"""Plot the loss fucntion using matplotlib"""

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
model.eval()
with torch.no_grad():
    outputs = model(X_test)
    _, predicted = torch.max(outputs, 1)

    accuracy = accuracy_score(y_test, predicted)
    precision = precision_score(y_test, predicted, average='weighted')
    recall = recall_score(y_test, predicted, average='weighted')
    f1 = f1_score(y_test, predicted, average='weighted')

    print(f'Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1 Score: {f1:.2f}')

"""send a sample tweet to check if it works"""

new_text = ["@Jason_Gio meh. :P thanks for the heads up, but not too concerned about another angry dude on twitter."]
new_text_vectorized = vectorizer.transform(new_text).toarray()
new_text_tensor = torch.tensor(new_text_vectorized, dtype=torch.float32)

model.eval()
with torch.no_grad():
    outputs = model(new_text_tensor)
    _, predicted = torch.max(outputs, 1)
predicted_class = predicted.item()

print(f'Predicted class: {predicted_class}')

"""# **RNN implementation**

Better than simple ANN as it "remembers" the previous state as well. In this RNN, the data runs only forward i.e it is unidorectional. It is better for sequential data such as sentences whose meaning depends on the whole sentence.
"""

class RNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(RNNModel, self).__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(num_layers, x.size(0), hidden_dim).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])
        return out


input_dim = X_train.shape[1]
hidden_dim = 128
output_dim = len(set(y))
num_layers = 2

model = RNNModel(input_dim, hidden_dim, output_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training the model
losses = []
epochs = 10
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train.unsqueeze(1))
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
model.eval()
with torch.no_grad():
    outputs = model(X_test.unsqueeze(1))
    _, predicted = torch.max(outputs, 1)

    accuracy = accuracy_score(y_test, predicted)
    precision = precision_score(y_test, predicted, average='weighted')
    recall = recall_score(y_test, predicted, average='weighted')
    f1 = f1_score(y_test, predicted, average='weighted')

    print(f'Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1 Score: {f1:.2f}')

new_text = ["old people are the worst"]
new_text_vectorized = vectorizer.transform(new_text).toarray()
new_text_tensor = torch.tensor(new_text_vectorized, dtype=torch.float32).unsqueeze(1)

model.eval()
with torch.no_grad():
    outputs = model(new_text_tensor)
    _, predicted = torch.max(outputs, 1)
predicted_class = predicted.item()

print(f'Predicted class: {predicted_class}')

"""# **Bidirectional Recurrent Neural Network Implementation**#

In this type of RNN, the input sequence is bidirectional. So, it should be able to capture both past and future dependencies
"""

class BiRNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(BiRNNModel, self).__init__()
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        #The hidden layer size would be double now as outputs are coming from 2 directions now.
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])
        return out

input_dim = X_train.shape[1]
hidden_dim = 128
output_dim = len(set(y))
num_layers = 2

model = BiRNNModel(input_dim, hidden_dim, output_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

losses = []
epochs = 10
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train.unsqueeze(1))
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
model.eval()
with torch.no_grad():
    outputs = model(X_test.unsqueeze(1))
    _, predicted = torch.max(outputs, 1)

    accuracy = accuracy_score(y_test, predicted)
    precision = precision_score(y_test, predicted, average='weighted')
    recall = recall_score(y_test, predicted, average='weighted')
    f1 = f1_score(y_test, predicted, average='weighted')

    print(f'Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1 Score: {f1:.2f}')

new_text = [" don't have a problem w/ a white person saying nigga but when its dumb fucks like @ashleyheil2 saying nigger she needs 2 get that ass beat!"]
new_text_vectorized = vectorizer.transform(new_text).toarray()
new_text_tensor = torch.tensor(new_text_vectorized, dtype=torch.float32).unsqueeze(1)

model.eval()
with torch.no_grad():
    outputs = model(new_text_tensor)
    _, predicted = torch.max(outputs, 1)
predicted_class = predicted.item()

print(f'Predicted class: {predicted_class}')

"""# **Long Short-Term Memory Implementation**

This type of RNN is supposed to solve the probelm if the state influencing the context is not recent. LSTM has cells in hiddenlayers pf neural network. Which has 3 gates. This model requires much more resources and training time.
"""

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, x):
        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)
        c0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out


input_dim = X_train.shape[1]
hidden_dim = 128
output_dim = len(set(y))
num_layers = 2

model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

losses = []
epochs = 10
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train.unsqueeze(1))
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
model.eval()
with torch.no_grad():
    outputs = model(X_test.unsqueeze(1))
    _, predicted = torch.max(outputs, 1)

    accuracy = accuracy_score(y_test, predicted)
    precision = precision_score(y_test, predicted, average='weighted')
    recall = recall_score(y_test, predicted, average='weighted')
    f1 = f1_score(y_test, predicted, average='weighted')

    print(f'Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1 Score: {f1:.2f}')

new_text = ["Fuck you ugly dumb nigger name Wayne off my toys ugly trash"]
new_text_vectorized = vectorizer.transform(new_text).toarray()
new_text_tensor = torch.tensor(new_text_vectorized, dtype=torch.float32).unsqueeze(1)

model.eval()
with torch.no_grad():
    outputs = model(new_text_tensor)
    _, predicted = torch.max(outputs, 1)
predicted_class = predicted.item()

print(f'Predicted class: {predicted_class}')

"""#**Gated recurrent units (GRUs) implementation**

designed to handle the vanishing gradient problem and can capture longer dependencies without the complexity of LSTM. GRU is often more efficient than LSTM because it has fewer parameters, which can result in faster training while still maintaining good performance.
"""

class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply hidden_dim by 2 for bidirectional

    def forward(self, x):
        h0 = torch.zeros(num_layers * 2, x.size(0), hidden_dim).to(x.device)  # Initial hidden state for both directions
        out, _ = self.gru(x, h0)  # Forward pass through GRU
        out = self.fc(out[:, -1, :])  # Output from the last time step of both directions
        return out
input_dim = X_train.shape[1]
hidden_dim = 128
output_dim = len(set(y))
num_layers = 2

model = GRUModel(input_dim, hidden_dim, output_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

losses = []
epochs = 10
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train.unsqueeze(1))
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())
    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
model.eval()
with torch.no_grad():
    outputs = model(X_test.unsqueeze(1))
    _, predicted = torch.max(outputs, 1)

    accuracy = accuracy_score(y_test, predicted)
    precision = precision_score(y_test, predicted, average='weighted')
    recall = recall_score(y_test, predicted, average='weighted')
    f1 = f1_score(y_test, predicted, average='weighted')

    print(f'Accuracy: {accuracy * 100:.2f}%')
    print(f'Precision: {precision:.2f}')
    print(f'Recall: {recall:.2f}')
    print(f'F1 Score: {f1:.2f}')

new_text = ["Bro. U gotta chill RT @CHILLShrammy: Dog FUCK KP that dumb nigger bitch lmao"]
new_text_vectorized = vectorizer.transform(new_text).toarray()
new_text_tensor = torch.tensor(new_text_vectorized, dtype=torch.float32).unsqueeze(1)

model.eval()
with torch.no_grad():
    outputs = model(new_text_tensor)
    _, predicted = torch.max(outputs, 1)
predicted_class = predicted.item()

print(f'Predicted class: {predicted_class}')